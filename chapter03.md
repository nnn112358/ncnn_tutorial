# 第3章 モデル変換の基礎

## 3.1 変換フローの全体像
ncnnで推論を行うには、まず学習済みモデルをncnn専用の`param`ファイルと`bin`ファイルに変換する必要がある。一般的な流れは、(1) 学習フレームワークからONNXへエクスポート、(2) `onnx2ncnn`でncnnの中間形式に変換、(3) `ncnnoptimize`で層のまとめやメモリ削減を行い、必要に応じて(4) `ncnn2mem`でバイナリをメモリマップ向けに変換するという段階を踏む。本章ではこれらの各ステップを詳細に解説し、よくあるエラーやデバッグの勘所を整理する。

## 3.2 ONNXへのエクスポート
PyTorchでは`torch.onnx.export`、TensorFlowでは`tf2onnx.convert`や`tensorflow-onnx`を使ってモデルをONNX形式へ変換する。動的入力を扱う場合は`dynamic_axes`や`--opset`オプションを正しく設定し、ncnnがサポートしているONNX opset（通常は11〜13）を選ぶのが安全だ。エクスポート時にはBatchNormの統合や`eval()`モードへの切り替えを忘れずに行い、推論時と同じ挙動を再現できる状態でONNXを生成する。出力されたONNXファイルは`netron.app`などのビューワで確認し、層構造が期待通りかを検証する。

## 3.3 onnx2ncnnによる初期変換
ncnnに付属する`onnx2ncnn`はONNXモデルをncnnの`param`と`bin`に分割するツールだ。実行コマンドは次の通りである。
```bash
onnx2ncnn model.onnx model.param model.bin
```
この段階ではONNX特有の算術演算や制御フローがncnnの既存レイヤーへマッピングされる。もし対応していない演算が含まれている場合はエラーが表示されるため、オンプレの算術を単純化するか、後述するカスタムレイヤーを用意する。変換ログには各レイヤーの名称や入出力形状が記録されるので、アプリケーション側で利用するノード名を把握するのに役立つ。

## 3.4 ncnnoptimizeでの最適化
`ncnnoptimize`は推論時のメモリ効率と速度を高めるツールで、冗長なレイヤーの結合、定数畳み込みの事前計算、Half精度への変換などを行う。
```bash
ncnnoptimize model.param model.bin model_opt.param model_opt.bin 65536
```
末尾の`65536`はワークスペースのメモリ上限で、ターゲットデバイスのRAMに合わせて調整する。最適化後の`param`はレイヤー数が減り、畳み込みやアクティベーションが一体化されることが多い。変換後は`diff`や`netron`で旧モデルと比較し、意図しないレイヤー削除が起きていないか確認する。

## 3.5 量子化と軽量化の選択肢
ncnnは`quantize`ツールでINT8量子化をサポートしており、校正データセットを指定して最適なスケールを計算する。
```bash
ncnn2table model.param model.bin image_list.txt table
quantize model.param model.bin table model_int8.param model_int8.bin
```
量子化により推論速度とメモリ使用量が大幅に改善される一方、精度低下のリスクがある。校正データは本番分布を代表するものを選び、生成された量子化モデルを検証セットで評価する。さらにチャンネルプルーニングやKnowledge Distillationといった軽量化を学習段階で施し、ncnn側では`folded_bn`などの最適化オプションと組み合わせると効果的だ。

## 3.6 変換結果の検証
変換後のモデルは必ずパイプラインの各段階で出力を比較し、精度が維持されているかを確認する。PyTorchの`model(input)`出力とncnnの推論結果を比較するために、同一のリファレンス入力を用意し、浮動小数点誤差の許容範囲内で一致するかを調べる。誤差が大きい場合はONNXへの変換時に演算が近似化されていないか、`onnx2ncnn`で未対応レイヤーがスキップされていないかを疑う。`ncnn2mem`でメモリマップ形式に変換した場合も同様に検証する。

## 3.7 トラブルシューティング
よくある問題として、ONNX出力に`Unsupported`と表示される演算が含まれるケースがある。この場合は学習フレームワーク側でレイヤーを置換するか、ncnnにカスタムレイヤーを追加する。また、バッチサイズ1を前提にした実装が多いため、動的形状を扱う場合は`reshape`や`permute`の扱いに注意する。`ncnnoptimize`適用後に推論が失敗する場合は、最適化前のモデルで問題ないかを検証し、問題が再現するレイヤーを特定する。ログ出力を増やす、`Extractor`で中間テンソルを取り出すなどしてデバッグを行うと原因に辿り着きやすい。

## 3.8 本章のまとめ
本章では学習済みモデルをncnnで扱うための変換手順を解説した。ONNXへのエクスポートから`onnx2ncnn`、`ncnnoptimize`、量子化ツールの利用方法、検証やトラブルシューティングまでを通して、安定した推論パイプラインの構築に必要な知識を整理した。次章ではncnn内部の推論エンジンがどのようにレイヤーを実行するか、アーキテクチャを掘り下げて理解する。
