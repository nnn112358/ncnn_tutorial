# 3. ncnnの基本概念

## 3.1 ncnnアーキテクチャ概要

ncnnは、効率的なニューラルネットワーク推論のために設計された独特のアーキテクチャを持っています。理解すべき主要な構成要素は以下の通りです：

### 主要コンポーネント

ncnnアーキテクチャの中核を成すのが**ncnn::Net**クラスです。このクラスは、ニューラルネットワーク全体を表現するメインクラスとして機能し、モデルファイルの読み込み、ネットワーク構造の管理、そして推論処理の実行調整を担当しています。重要な点として、ncnn::Netクラスはスレッドセーフではないため、マルチスレッド環境で並列処理を行う場合は、スレッドごとに独立したインスタンスを作成する必要があります。

テンソルデータの管理には**ncnn::Mat**クラスが使用されます。このクラスは、1次元から4次元までの多次元データを効率的に格納できるマトリックスクラスとして設計されており、メモリ効率を最優先に考えた実装が特徴です。画像データ、重みパラメータ、中間計算結果など、推論処理で扱われるすべてのデータがこのクラスで管理されます。

実際の推論処理を実行するのが**ncnn::Extractor**クラスです。このクラスは、ncnn::Netから生成される推論実行エンジンであり、入力データの設定から出力データの取得まで、推論プロセス全体を制御します。一つのncnn::Netインスタンスから複数のExtractorを生成することが可能で、これにより異なる入力に対する並列処理が実現できます。

各種ニューラルネットワーク演算を実装する基底クラスが**ncnn::Layer**です。Convolution（畳み込み）、ReLU、Pooling等の具体的な演算は、すべてこのクラスを継承して実装されています。この設計により、新しい演算子の追加や既存演算の最適化が柔軟に行えるようになっています。

### アーキテクチャの特徴

**メモリ効率重視の設計**:
```
入力テンソル → Layer1 → 中間テンソル → Layer2 → 出力テンソル
                ↓              ↓
            in-place演算    メモリ再利用
```

ncnnでは可能な限りin-place演算を採用し、不要になったメモリを即座に再利用することで、メモリ使用量を最小限に抑えています。

**計算グラフの最適化**についても、ncnnの重要な特徴の一つです。演算子融合（Operator Fusion）技術により、連続する演算を統合して実行回数を削減し、大幅な高速化を実現しています。また、推論時に不要となる演算の自動除去や、CPUキャッシュ効率を考慮したメモリアクセスパターンの最適化により、限られたリソースでも最大限の性能を引き出すことができます。

## 3.2 モデルファイル形式

ncnnでは、ニューラルネットワークモデルを2つのファイルで表現します：

### 3.2.1 .paramファイル

`.param`ファイルは、ネットワークの構造を定義するテキストファイルです。

**基本構造**:
```
7767517
13 13
Input            input                    0 1 input
Convolution      conv1                    1 1 input conv1 0=64 1=7 11=7 2=1 12=1 3=2 13=2 4=3 14=3 15=3 16=3 5=1 6=9408
ReLU             relu1                    1 1 conv1 relu1 0=0.000000
Pooling          pool1                    1 1 relu1 pool1 0=0 1=3 11=3 2=2 12=2 3=0 13=0 14=0
Convolution      conv2                    1 1 pool1 conv2 0=192 1=5 11=5 2=1 12=1 3=2 13=2 4=2 14=2 15=2 16=2 5=1 6=307200
```

**ファイル形式の詳細**について説明します。1行目にはマジックナンバー（`7767517`）が記録されており、これによってncnnはファイルが正しい形式であることを確認しています。2行目には、ネットワーク全体のレイヤー数とブロブ数が記載されており、ファイル解析時の事前情報として使用されます。3行目以降には、各レイヤーの詳細な定義が順次記述されています。

**レイヤー定義の構文**:
```
レイヤータイプ レイヤー名 入力数 出力数 入力ブロブ名 出力ブロブ名 パラメータ
```

**主要なレイヤータイプとパラメータ**:

| レイヤータイプ | 主要パラメータ | 説明 |
|---------------|---------------|------|
| `Convolution` | `0=出力チャンネル数`, `1=カーネルサイズ`, `2=ストライド`, `3=パディング` | 畳み込み層 |
| `ReLU` | `0=負の傾き` | ReLU活性化関数 |
| `Pooling` | `0=プール種別`, `1=カーネルサイズ`, `2=ストライド` | プーリング層 |
| `InnerProduct` | `0=出力サイズ`, `1=バイアス有無`, `2=重み初期化` | 全結合層 |

### 3.2.2 .binファイル

`.bin`ファイルは、ネットワークの重みとバイアスを格納するバイナリファイルです。

**データ格納形式**については、現代的なプロセッサで一般的なリトルエンディアン形式が採用されています。数値データは32bit浮動小数点数（float32）がデフォルトとして使用されており、高精度な推論処理を保証しています。一方、量子化が適用されたモデルでは、メモリ効率と計算速度の向上を目的として、8bit整数や16bit浮動小数点数も使用されることがあります。

**ファイル構造**:
```
[Layer1の重みデータ] [Layer1のバイアスデータ] [Layer2の重みデータ] ...
```

各レイヤーのデータは、.paramファイルで定義された順序で格納されています。

**データ配置の例（Convolutionレイヤー）**:
```
重み: [出力チャンネル][入力チャンネル][高さ][幅]
バイアス: [出力チャンネル]
```

## 3.3 レイヤーとオペレーター

ncnnでは100種類以上のレイヤー（オペレーター）をサポートしており、主要なディープラーニングモデルの大部分を実行できます。

### 基本的なレイヤー

**Convolution（畳み込み）**レイヤーは、画像認識の中核となる演算です。ncnnでは標準的な2D畳み込みに加えて、計算効率を重視したDepthwise ConvolutionやGrouped Convolution、受容野を拡張するDilated Convolutionなど、様々な畳み込み手法をサポートしています。これらの実装は、各種プロセッサの特性に合わせて最適化されており、高速な推論処理を実現しています。

```cpp
// 例：3x3畳み込み、入力32ch、出力64ch、ストライド1、パディング1
// パラメータ: 0=64 1=3 2=1 3=1 5=1 6=18432
```

**活性化関数**については、従来のReLU、ReLU6、LeakyReLUから、より表現力の高いSwish、Mish、GELU等の現代的な活性化関数まで幅広くサポートされています。また、非線形変換を行うSigmoidやTanhも利用可能で、様々なモデルアーキテクチャに対応できます。

**プーリング**演算では、特徴マップのサイズを削減するMaxPoolingとAveragePooling、グローバル特徴を抽出するGlobalAveragePooling、出力サイズを柔軟に制御できるAdaptivePoolingなどが実装されています。これらの演算により、計算量の削減と重要な特徴の保持を両立しています。

**正規化**レイヤーとしては、学習の安定化に重要な役割を果たすBatchNorm、スタイル変換等で使用されるInstanceNorm、Transformerアーキテクチャで重要なLayerNormなどが提供されており、現代的なニューラルネットワークの構築に必要な要素が揃っています。

### 高度なレイヤー

**注意機構（Attention）**については、近年の深層学習において重要性が高まっている演算です。ncnnではMultiHeadAttentionやSelfAttentionといった、Transformerアーキテクチャの中核となる注意機構をサポートしており、自然言語処理や画像認識の最新モデルを効率的に実行できます。

**リカレント**演算としては、時系列データ処理に不可欠なLSTM、計算効率に優れたGRU、そして基本的なRNNが実装されています。これらの演算により、音声認識や自然言語処理といった、時間軸方向の依存関係を考慮する必要があるタスクに対応できます。

**カスタムレイヤー**の実装も可能で、独自の演算を実装したい場合は、ncnn::Layerを継承してカスタムレイヤーを作成できます。これにより、特殊な要求や最新の研究成果を取り入れた演算を、ncnnフレームワーク内で効率的に実行することができます。

### レイヤーの最適化

ncnnでは、実行時にレイヤーの最適化が自動的に行われます：

**演算子融合**:
```
Convolution + BatchNorm + ReLU → ConvolutionReLU
```

**量子化**:
```
float32演算 → int8演算（最大8倍高速化）
```

## 3.4 メモリ管理

ncnnの高性能を支える重要な要素の一つが、効率的なメモリ管理システムです。

### メモリアロケーター

**ncnn::Allocator**は、高性能なメモリ管理を実現するための中核的なコンポーネントです。メモリプールベースの設計により、従来のmalloc/freeによる動的メモリ割り当てと比較して、大幅に高速なメモリ割り当てを実現しています。また、メモリプールの使用により、メモリフラグメンテーションの問題を効果的に防止し、長時間の実行においても安定したパフォーマンスを維持できます。さらに、プロセッサのキャッシュ効率を考慮したメモリ配置により、データアクセス時のキャッシュミスを最小限に抑え、全体的な実行効率を向上させています。

```cpp
// カスタムアロケーターの設定例
ncnn::UnlockedPoolAllocator allocator;
net.opt.blob_allocator = &allocator;
net.opt.workspace_allocator = &allocator;
```

### メモリレイアウト

**ncnn::Mat のメモリレイアウト**:

```cpp
// 4次元テンソル [N, C, H, W] の場合
// N: バッチサイズ（通常1）
// C: チャンネル数
// H: 高さ
// W: 幅

ncnn::Mat tensor(w, h, c);  // 3次元テンソル [C, H, W]
```

**データアクセスパターン**:
```cpp
// 要素へのアクセス
float value = tensor.channel(c).row(h)[w];

// または
float* ptr = tensor.channel(c);
float value = ptr[h * w + w];
```

### in-place演算

メモリ効率を最大化するため、ncnnでは多くの演算がin-placeで実行されます：

```cpp
// 入力テンソル自体が変更される
ReLU(input_tensor);  // input_tensor の値が直接変更される
```

### メモリ最適化のベストプラクティス

効率的なメモリ使用のためには、適切なメモリプールサイズの設定が重要です。使用するモデルの複雑さと利用可能なシステムメモリを考慮して、適切なワークスペースサイズを設定することで、メモリ不足エラーを防ぎつつ最適なパフォーマンスを実現できます。

```cpp
// メモリプールサイズの調整
net.opt.workspace_size_mb = 256;  // 256MBのワークスペース
```

不要なテンソルの早期解放も重要な最適化手法です。C++のスコープ機能を活用して、一時的なテンソルオブジェクトが適切なタイミングで自動解放されるようにコードを設計することで、メモリ使用量を最小限に抑えることができます。

```cpp
{
    ncnn::Mat temp_tensor = some_operation();
    // temp_tensor は自動的にスコープ外で解放される
}
```

バッチサイズの最適化については、ncnnではバッチサイズ1での推論が最も効率的に動作するよう設計されています。これは、モバイルやエッジデバイスでの単一サンプル処理を想定した最適化の結果です。

```cpp
// バッチサイズ1での推論が最も効率的
ncnn::Mat input(224, 224, 3);  // [3, 224, 224]
```

### プロファイリングとデバッグ

メモリ使用量の監視：

```cpp
// メモリ使用量の取得
size_t memory_usage = allocator.get_memory_usage();
printf("Memory usage: %.2f MB\n", memory_usage / 1024.0 / 1024.0);
```

ncnnの基本概念を理解することで、効率的なディープラーニング推論アプリケーションを開発する基盤が整います。次章では、他のフレームワークで学習したモデルをncnn形式に変換する方法について詳しく学習します。