# 第1章 ncnnとは

## 1.1 ncnn誕生の背景
2017年、TencentのYoutu Labがスマートフォン向け高性能なディープラーニング推論基盤としてncnnを公開した。中国国内の膨大なユーザーを抱えるSNSやフィンテック製品でAI機能を素早く提供する必要があり、サーバーサイドではなく端末内でリアルタイムに動作する軽量推論エンジンが求められた。クラウド依存の遅延や通信コスト、個人情報の扱いといった制約から、端末側で完結する推論インフラが急務となり、これがncnnの原動力となった。

## 1.2 プロジェクトの目的と設計思想
ncnnは「高速・軽量・移植性」を最優先に設計され、依存ライブラリを最小限に抑えてC++のみで実装されている。ビルド済みバイナリのサイズは数MB以下に収まるよう設計され、組み込みLinuxやRTOSでも容易に利用できる。このフットプリントの小ささはヘッダオンリー構造とスタティックリンクの柔軟性によって支えられている。SIMD最適化を徹底し、ARM NEONやx86 SSE/AVX命令セットへ対応する手作業最適化が随所に施されている。さらにカスタムメモリアロケータでバッファ再利用を行い、動的メモリ割り当てを最小限に抑えている。これらによりフレームワーク自身が持つオーバーヘッドを削ぎ落とし、推論結果までのレイテンシを最大限短縮している。

## 1.3 オープンソースコミュニティとライセンス
ncnnはBSD 3-Clauseライセンスで公開され、商用製品に組み込んでもライセンス義務が軽い。GitHubリポジトリではTencentのコアメンバーに加え、世界中の開発者がバグ修正や最適化パッチを提供する活発なコミュニティが形成されている。IssueやPull Requestを通じた議論が日常的に行われ、新しいプロセッサアーキテクチャへの対応、ONNXやTensorFlow Liteといった外部形式との連携強化、量子化・プルーニング等の軽量化機能の追加が継続的に進んでいる。

## 1.4 他フレームワークとの比較
TensorFlow LiteやPyTorch Mobileと比べ、ncnnは推論専用に機能を絞り込んだことで高速性と軽量性が際立つ。CUDAやOpenCLなどGPUアクセラレーションに重きを置くフレームワークに対し、ncnnはCPUベースでも高性能を発揮できるアーキテクチャに特化している。一方で学習機能を内包せず、モデル圧縮や量子化パイプラインは外部ツールと連携する前提となっているため、学習からデプロイまで一貫させたい場合は他のツールチェーンと組み合わせる必要がある。これがncnnを「最終推論のためのエンジン」と位置付ける理由であり、用途に応じてフレームワークを使い分ける設計哲学が見て取れる。

## 1.5 アーキテクチャ概要
ncnnは`Net`クラスを中心に、レイヤー定義・パラメータ読み込み・推論実行を行うシンプルな構造を持つ。`ParamDict`と`ModelBin`でモデル構成と重みを読み込み、`Extractor`オブジェクトで入出力テンソルを扱う。バックエンドはスレッドプールやワークロードスケジューラを備え、各CPUコアに演算を分配する。さらにVulkanバックエンドを有効化すればGPU計算資源も活用でき、Androidスマートフォンや一部の組み込みGPUで推論速度を伸ばせる。アーキテクチャ全体はプラグイン可能なレイヤー設計になっており、ユーザーは独自演算を`Layer`派生クラスとして実装できる。

## 1.6 主な適用分野
画像分類、物体検出、顔認識、文書スキャン、音声処理など、スマートフォン向けAI機能で多用されている。特にWeChatやQQなどTencentの製品群で採用され、リアルタイムARフィルタ、顔交換、ビューティーカメラ、OCRなどの体験を支えている。また、ドローンや監視カメラといったエッジデバイスでも多用され、クラウド接続の難しい環境での自律推論を実現している。医療や産業検査分野でもカメラ画像解析をローカルで処理する用途に広がっており、低消費電力で高スループットを実現できる点が評価されている。

## 1.7 本書全体の位置づけと本章のまとめ
本書ではncnnを中心に、モデルの変換・軽量化から、端末へのデプロイ、性能最適化、カスタムレイヤー開発まで実践的な知識を段階的に解説していく。本章ではncnnが生まれた背景、設計思想、アーキテクチャ、適用分野を概観した。次章では実際にncnnを動かすための環境構築とツールチェーンを整え、開発準備を整える。
